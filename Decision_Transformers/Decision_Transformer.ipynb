{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJnwpdtXTuF7"
      },
      "source": [
        "# Training Decision Transformers to interact with Halfcheetah environment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Decision Transformer model was introduced by [“Decision Transformer: Reinforcement Learning via Sequence Modeling”](https://arxiv.org/abs/2106.01345) by Chen L. et al. It abstracts Reinforcement Learning as a conditional-sequence modeling problem. The fundamental idea is to use a sequence modeling algorithm, using Transformers, instead of training a policy with usual reinforcement learning models. Given the desired return, past states, and actions, the Transformer will generate future actions to achieve this desired return. It’s an autoregressive model conditioned on the desired return, past states, and actions to generate future actions that achieve the desired return. This is a complete shift in the Reinforcement Learning paradigm since we use generative trajectory modeling (modeling the joint distribution of the sequence of states, actions, and rewards) to replace conventional RL algorithms. It means that in Decision Transformers, we don’t maximize the return but rather generate a series of future actions that achieve the desired return.\n",
        "\n",
        "In this example we employ the capabilities of the HuggingFace library that has a particular class for this task.The process is the following:\n",
        "\n",
        "\n",
        "\n",
        "1.   We feed the last K timesteps into the Decision Transformer with three inputs:\n",
        "      *   Return-to-go\n",
        "      *   State\n",
        "      *   Action\n",
        "2.  The tokens are embedded either with a linear layer if the state is a vector or a CNN encoder if it’s frames.\n",
        "\n",
        "3.  The inputs are processed by a GPT-2 model, which predicts future actions via autoregressive modeling.\n",
        "\n",
        "Specifically in our case we don't actually learn by interacting with environment but we learn from data collected by other agents or humans. This approach is usually called offline RL.\n",
        "\n",
        "If you want to train it, is suggest the use of a GPU."
      ],
      "metadata": {
        "id": "Jb7p99cUztlB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBTys2Ctv7zQ"
      },
      "source": [
        "- `Hardware Accelerator > GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCsT6KVxwARY"
      },
      "source": [
        "### Install dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr-1h83zVsMK"
      },
      "source": [
        "### Install and import the packages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common \\\n",
        "    patchelf \\\n",
        "    xvfb"
      ],
      "metadata": {
        "id": "kE4nvXb8mlQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuXIPJUWVrhq"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.21.0\n",
        "!pip install free-mujoco-py\n",
        "!pip install mujoco\n",
        "!pip install gym[mujoco]\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets==2.14.5\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.5\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install xvfbwrapper\n",
        "!pip install huggingface_hub\n",
        "!pip install pyarrow==14.0.2\n",
        "!pip install requests==2.31.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1HQOkZjl2rb"
      },
      "outputs": [],
      "source": [
        "!pip install gym==0.21.0\n",
        "!pip install free-mujoco-py\n",
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install imageio==2.9.0\n",
        "!pip install imageio-ffmpeg==0.4.5\n",
        "!pip install colabgymrender==1.0.2\n",
        "!pip install xvfbwrapper\n",
        "!pip install huggingface_hub\n",
        "!pip install pyarrow==14.0.2\n",
        "!pip install requests==2.31.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mujoco"
      ],
      "metadata": {
        "id": "K-icSkuLjE5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[mujoco]"
      ],
      "metadata": {
        "id": "WfR2A1jFjGlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DktITQNXTopc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import DecisionTransformerConfig, DecisionTransformerModel, Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1Ugq2POUmRA"
      },
      "source": [
        "### Loading the dataset from the HuggingFace Hub and instantiating the model\n",
        "\n",
        "HuggingFace repository is huge and for this example we will be training with the halfcheetah “expert” dataset, hosted on hub.\n",
        "\n",
        "First we need to import the load_dataset function from the HuggingFace datasets package and download the dataset to our machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3bLeIHqUwq7"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\" # we diable weights and biases logging for this tutorial\n",
        "dataset = load_dataset(\"edbeeching/decision_transformer_gym_replay\", \"halfcheetah-expert-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFmTdHoHUD13"
      },
      "source": [
        "### Defining a custom DataCollator for the transformers Trainer class"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case we wish to match the [author's implementation](https://github.com/kzl/decision-transformer), that is we need to perform some additional processing to the data as follows:\n",
        "\n",
        "* Normalize each feature by subtracting the mean and dividing by the standard deviation.\n",
        "* Pre-compute discounted returns for each trajectory.\n",
        "* Scale the rewards and returns by a factor of 1000.\n",
        "* Augment the dataset sampling distribution so it takes into account the length of the expert agent’s trajectories."
      ],
      "metadata": {
        "id": "4MOIuYJqhMab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to perform this dataset preprocessing, we will use a custom HuggingFace [Data Collator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#data-collator).\n",
        "Data Collator are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset.\n",
        "\n",
        "To be able to build batches, data collators may apply some processing (like padding). Some of them (like DataCollatorForLanguageModeling) also apply some random data augmentation (like random masking) on the formed batch."
      ],
      "metadata": {
        "id": "LsbqLW9Thur3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1QzZHmPUM4p"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DecisionTransformerGymDataCollator:\n",
        "    return_tensors: str = \"pt\"\n",
        "    max_len: int = 20 #subsets of the episode we use for training\n",
        "    state_dim: int = 17  # size of state space\n",
        "    act_dim: int = 6  # size of action space\n",
        "    max_ep_len: int = 1000 # max episode length in the dataset\n",
        "    scale: float = 1000.0  # normalization of rewards/returns\n",
        "    state_mean: np.array = None  # to store state means\n",
        "    state_std: np.array = None  # to store state stds\n",
        "    p_sample: np.array = None  # a distribution to take account trajectory lengths\n",
        "    n_traj: int = 0 # to store the number of trajectories in the dataset\n",
        "\n",
        "    def __init__(self, dataset) -> None:\n",
        "        self.act_dim = len(dataset[0][\"actions\"][0])\n",
        "        self.state_dim = len(dataset[0][\"observations\"][0])\n",
        "        self.dataset = dataset\n",
        "        # calculate dataset stats for normalization of states\n",
        "        states = []\n",
        "        traj_lens = []\n",
        "        for obs in dataset[\"observations\"]:\n",
        "            states.extend(obs)\n",
        "            traj_lens.append(len(obs))\n",
        "        self.n_traj = len(traj_lens)\n",
        "        states = np.vstack(states)\n",
        "        self.state_mean, self.state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
        "\n",
        "        traj_lens = np.array(traj_lens)\n",
        "        self.p_sample = traj_lens / sum(traj_lens)\n",
        "\n",
        "    def _discount_cumsum(self, x, gamma):\n",
        "        discount_cumsum = np.zeros_like(x)\n",
        "        discount_cumsum[-1] = x[-1]\n",
        "        for t in reversed(range(x.shape[0] - 1)):\n",
        "            discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
        "        return discount_cumsum\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch_size = len(features)\n",
        "        # this is a bit of a hack to be able to sample of a non-uniform distribution\n",
        "        batch_inds = np.random.choice(\n",
        "            np.arange(self.n_traj),\n",
        "            size=batch_size,\n",
        "            replace=True,\n",
        "            p=self.p_sample,  # reweights so we sample according to timesteps\n",
        "        )\n",
        "        # a batch of dataset features\n",
        "        s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
        "\n",
        "        for ind in batch_inds:\n",
        "            # for feature in features:\n",
        "            feature = self.dataset[int(ind)]\n",
        "            si = random.randint(0, len(feature[\"rewards\"]) - 1)\n",
        "\n",
        "            # get sequences from dataset\n",
        "            s.append(np.array(feature[\"observations\"][si : si + self.max_len]).reshape(1, -1, self.state_dim))\n",
        "            a.append(np.array(feature[\"actions\"][si : si + self.max_len]).reshape(1, -1, self.act_dim))\n",
        "            r.append(np.array(feature[\"rewards\"][si : si + self.max_len]).reshape(1, -1, 1))\n",
        "\n",
        "            d.append(np.array(feature[\"dones\"][si : si + self.max_len]).reshape(1, -1))\n",
        "            timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
        "            timesteps[-1][timesteps[-1] >= self.max_ep_len] = self.max_ep_len - 1  # padding cutoff\n",
        "            rtg.append(\n",
        "                self._discount_cumsum(np.array(feature[\"rewards\"][si:]), gamma=1.0)[\n",
        "                    : s[-1].shape[1]\n",
        "                ].reshape(1, -1, 1)\n",
        "            )\n",
        "            if rtg[-1].shape[1] < s[-1].shape[1]:\n",
        "                print(\"if true\")\n",
        "                rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
        "\n",
        "            # padding and state + reward normalization\n",
        "            tlen = s[-1].shape[1]\n",
        "            s[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, self.state_dim)), s[-1]], axis=1)\n",
        "            s[-1] = (s[-1] - self.state_mean) / self.state_std\n",
        "            a[-1] = np.concatenate(\n",
        "                [np.ones((1, self.max_len - tlen, self.act_dim)) * -10.0, a[-1]],\n",
        "                axis=1,\n",
        "            )\n",
        "            r[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), r[-1]], axis=1)\n",
        "            d[-1] = np.concatenate([np.ones((1, self.max_len - tlen)) * 2, d[-1]], axis=1)\n",
        "            rtg[-1] = np.concatenate([np.zeros((1, self.max_len - tlen, 1)), rtg[-1]], axis=1) / self.scale\n",
        "            timesteps[-1] = np.concatenate([np.zeros((1, self.max_len - tlen)), timesteps[-1]], axis=1)\n",
        "            mask.append(np.concatenate([np.zeros((1, self.max_len - tlen)), np.ones((1, tlen))], axis=1))\n",
        "\n",
        "        s = torch.from_numpy(np.concatenate(s, axis=0)).float()\n",
        "        a = torch.from_numpy(np.concatenate(a, axis=0)).float()\n",
        "        r = torch.from_numpy(np.concatenate(r, axis=0)).float()\n",
        "        d = torch.from_numpy(np.concatenate(d, axis=0))\n",
        "        rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).float()\n",
        "        timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).long()\n",
        "        mask = torch.from_numpy(np.concatenate(mask, axis=0)).float()\n",
        "\n",
        "        return {\n",
        "            \"states\": s,\n",
        "            \"actions\": a,\n",
        "            \"rewards\": r,\n",
        "            \"returns_to_go\": rtg,\n",
        "            \"timesteps\": timesteps,\n",
        "            \"attention_mask\": mask,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmTRGPKYUVFG"
      },
      "source": [
        "### Extending the Decision Transformer Model to include a loss function\n",
        "\n",
        "In order to train the model with the HuggingFace trainer class, we first need to ensure the dictionary it returns contains a loss, in this case L-2 norm of the models action predictions and the targets.\n",
        "The L-2 norm loss is given by:\n",
        "\n",
        "$$\\text{Loss} = \\| \\hat{a} - a \\|_2 = \\sqrt{\\sum_{i=1}^{n} (\\hat{a}_i - a_i)^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwZp7hhFUh5u"
      },
      "outputs": [],
      "source": [
        "class TrainableDT(DecisionTransformerModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        output = super().forward(**kwargs)\n",
        "        # add the DT loss\n",
        "        action_preds = output[1]\n",
        "        action_targets = kwargs[\"actions\"]\n",
        "        attention_mask = kwargs[\"attention_mask\"]\n",
        "        act_dim = action_preds.shape[2]\n",
        "        action_preds = action_preds.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "        action_targets = action_targets.reshape(-1, act_dim)[attention_mask.reshape(-1) > 0]\n",
        "\n",
        "        loss = torch.mean((action_preds - action_targets) ** 2)\n",
        "\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def original_forward(self, **kwargs):\n",
        "        return super().forward(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIJCY3b3pQAh"
      },
      "outputs": [],
      "source": [
        "collator = DecisionTransformerGymDataCollator(dataset[\"train\"])\n",
        "\n",
        "config = DecisionTransformerConfig(state_dim=collator.state_dim, act_dim=collator.act_dim)\n",
        "model = TrainableDT(config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "HfRyoZCPK1XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Architecture**\n",
        "\n",
        "1. Encoder (DecisionTransformerGPT2Model):\n",
        "This is the core of the model, based on the GPT-2 architecture but adapted for decision-making tasks. It includes:\n",
        "\n",
        " * Token and position embeddings (wte and wpe)\n",
        " * A dropout layer for regularization\n",
        " * A series of transformer blocks (3 in this case)\n",
        " * A final layer normalization (ln_f)\n",
        "\n",
        "  Each transformer block contains:\n",
        "\n",
        " * Two layer normalization layers (ln_1 and ln_2)\n",
        " * A self-attention mechanism (DecisionTransformerGPT2Attention)\n",
        " * A feedforward neural network (DecisionTransformerGPT2MLP)\n",
        "\n",
        "\n",
        "2. Embedding layers:\n",
        "\n",
        " * embed_timestep: Embeds the timestep information\n",
        " * embed_return: Transforms the return (reward) into an embedding\n",
        " * embed_state: Transforms the state into an embedding\n",
        " * embed_action: Transforms the action into an embedding\n",
        " * embed_ln: A layer normalization applied to the combined embeddings\n",
        "\n",
        "\n",
        "3. Prediction layers:\n",
        "\n",
        " * predict_state: Predicts the next state\n",
        " * predict_action: Predicts the next action (with a tanh activation)\n",
        " * predict_return: Predicts the expected return\n",
        "\n",
        "\n",
        "\n",
        "The model works by first embedding the input sequences (state, action, return, and timestep) into a common representation space. These embeddings are then processed through the transformer encoder, which allows the model to attend to different parts of the input sequence. Finally, the output of the encoder is used to make predictions about the next state, action, and expected return."
      ],
      "metadata": {
        "id": "-ahCKlRlqgpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nTotal trainable parameters: {count_parameters(model)}\")\n",
        "\n",
        "# You can also print the config\n",
        "print(\"\\nModel Configuration:\")\n",
        "print(config)"
      ],
      "metadata": {
        "id": "8aHAvgkiK6Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a view of the model parameters."
      ],
      "metadata": {
        "id": "kYl5pbsLsh_S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJJ2mr_cU4eE"
      },
      "source": [
        "### Defining the training hyperparameters and training the model\n",
        "Here, we define the training hyperparameters and our Trainer class that we'll use to train our Decision Transformer model.\n",
        "\n",
        "This step takes about an hour, so you may leave it running. It must be noted that authors have trained for at least 3 hours, so the results presented here are not as performant as the models hosted on the HuggingFace hub, but performance still quite good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNzzKWuuU9I4"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"output/\",\n",
        "    remove_unused_columns=False,\n",
        "    num_train_epochs=120,\n",
        "    per_device_train_batch_size=64,\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=1e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=0.25,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNaj6bOkp3bt"
      },
      "source": [
        "### Visualize the performance of the agent\n",
        "\n",
        "In this last part we will see how agent performs and interact with environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0NhIn4up26c"
      },
      "outputs": [],
      "source": [
        "import mujoco_py\n",
        "import gym\n",
        "\n",
        "from colabgymrender.recorder import Recorder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the model from checkpoint\n",
        "checkpoint_path = \"/content/output/checkpoint-1500\"\n",
        "config = DecisionTransformerConfig.from_pretrained(checkpoint_path)\n",
        "model = DecisionTransformerModel.from_pretrained(checkpoint_path, config=config)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "prQV935qgz9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-izl68BqUG5"
      },
      "outputs": [],
      "source": [
        "def get_action(model, states, actions, rewards, returns_to_go, timesteps):\n",
        "    # Get the device of the model\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Move all inputs to the same device as the model\n",
        "    states = states.to(device)\n",
        "    actions = actions.to(device)\n",
        "    returns_to_go = returns_to_go.to(device)\n",
        "    timesteps = timesteps.to(device)\n",
        "\n",
        "    states = states.reshape(1, -1, model.config.state_dim)\n",
        "    actions = actions.reshape(1, -1, model.config.act_dim)\n",
        "    returns_to_go = returns_to_go.reshape(1, -1, 1)\n",
        "    timesteps = timesteps.reshape(1, -1)\n",
        "\n",
        "    states = states[:, -model.config.max_length :]\n",
        "    actions = actions[:, -model.config.max_length :]\n",
        "    returns_to_go = returns_to_go[:, -model.config.max_length :]\n",
        "    timesteps = timesteps[:, -model.config.max_length :]\n",
        "    padding = model.config.max_length - states.shape[1]\n",
        "\n",
        "    # pad all tokens to sequence length\n",
        "    attention_mask = torch.cat([torch.zeros(padding, device=device), torch.ones(states.shape[1], device=device)])\n",
        "    attention_mask = attention_mask.to(dtype=torch.long, device=device).reshape(1, -1)\n",
        "    states = torch.cat([torch.zeros((1, padding, model.config.state_dim), device=device), states], dim=1).float()\n",
        "    actions = torch.cat([torch.zeros((1, padding, model.config.act_dim), device=device), actions], dim=1).float()\n",
        "    returns_to_go = torch.cat([torch.zeros((1, padding, 1), device=device), returns_to_go], dim=1).float()\n",
        "    timesteps = torch.cat([torch.zeros((1, padding), dtype=torch.long, device=device), timesteps], dim=1)\n",
        "\n",
        "    output = model(\n",
        "        states=states,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        returns_to_go=returns_to_go,\n",
        "        timesteps=timesteps,\n",
        "        attention_mask=attention_mask,\n",
        "        return_dict=True,\n",
        "    )\n",
        "\n",
        "    return output.action_preds[0, -1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFPuiNy-qWnP"
      },
      "outputs": [],
      "source": [
        "# build the environment\n",
        "directory = './video'\n",
        "model = model.to(device)\n",
        "env = gym.make(\"HalfCheetah-v3\")\n",
        "env = Recorder(env, directory, fps=30)\n",
        "max_ep_len = 1000\n",
        "scale = 1000.0  # normalization for rewards/returns\n",
        "TARGET_RETURN = 12000 / scale  # evaluation is conditioned on a return of 12000, scaled accordingly\n",
        "\n",
        "state_mean = collator.state_mean.astype(np.float32)\n",
        "state_std = collator.state_std.astype(np.float32)\n",
        "print(state_mean)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "act_dim = env.action_space.shape[0]\n",
        "\n",
        "# Create the decision transformer model\n",
        "state_mean = torch.from_numpy(state_mean).to(device=device)\n",
        "state_std = torch.from_numpy(state_std).to(device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Number of episodes to run\n",
        "num_episodes = 10\n",
        "\n",
        "# List to store episode returns\n",
        "episode_returns = []\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    episode_return, episode_length = 0, 0\n",
        "    state = env.reset()\n",
        "    target_return = torch.tensor(TARGET_RETURN, device=device, dtype=torch.float32).reshape(1, 1)\n",
        "    states = torch.from_numpy(state).reshape(1, state_dim).to(device=device, dtype=torch.float32)\n",
        "    actions = torch.zeros((0, act_dim), device=device, dtype=torch.float32)\n",
        "    rewards = torch.zeros(0, device=device, dtype=torch.float32)\n",
        "\n",
        "    timesteps = torch.tensor(0, device=device, dtype=torch.long).reshape(1, 1)\n",
        "    for t in range(max_ep_len):\n",
        "        actions = torch.cat([actions, torch.zeros((1, act_dim), device=device)], dim=0)\n",
        "        rewards = torch.cat([rewards, torch.zeros(1, device=device)])\n",
        "\n",
        "        action = get_action(\n",
        "            model,\n",
        "            (states - state_mean) / state_std,\n",
        "            actions,\n",
        "            rewards,\n",
        "            target_return,\n",
        "            timesteps,\n",
        "        )\n",
        "        actions[-1] = action\n",
        "        action = action.detach().cpu().numpy()\n",
        "\n",
        "        state, reward, done, _ = env.step(action)\n",
        "\n",
        "        cur_state = torch.from_numpy(state).to(device=device).reshape(1, state_dim)\n",
        "        states = torch.cat([states, cur_state], dim=0)\n",
        "        rewards[-1] = reward\n",
        "\n",
        "        pred_return = target_return[0, -1] - (reward / scale)\n",
        "        target_return = torch.cat([target_return, pred_return.reshape(1, 1)], dim=1)\n",
        "        timesteps = torch.cat([timesteps, torch.ones((1, 1), device=device, dtype=torch.long) * (t + 1)], dim=1)\n",
        "\n",
        "        episode_return += reward.item()  # Use .item() to get the scalar value\n",
        "        episode_length += 1\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Append the episode return to our list\n",
        "    episode_returns.append(episode_return)\n",
        "\n",
        "# Calculate mean and standard deviation of returns\n",
        "mean_reward = np.mean(episode_returns)\n",
        "std_reward = np.std(episode_returns)\n",
        "\n",
        "# Print the performance\n",
        "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "id": "iv4lkqvBTnN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1Zy7x9qbnI"
      },
      "outputs": [],
      "source": [
        "# Play the video\n",
        "env.play()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive('/content/checkpoint_folder', 'zip', '/content/output')\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/checkpoint_folder.zip')\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive('/content/DT_video', 'zip', '/content/video')\n",
        "files.download('/content/DT_video.zip')\n",
        "files.download('/content/__temp__.mp4')\n"
      ],
      "metadata": {
        "id": "M6nkQm9Q_AM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements_DT.txt"
      ],
      "metadata": {
        "id": "p4f9vHAbEN3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlPV2mkSHXBn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}